# both
initial_model_path: /home/test/testdata/models/Meta-Llama-3-8B-Instruct
initial_dataset_path: ""
dataset_type: math
mid_yaml_root_path: ./alignment-handbook/recipes/Llama3-8b/math_sft_dpo
check_point_root_path: ./checkpoints/math_sft_dpo
iteration_times: 3
port: 8000
devices: 7
tokenizer_first_path: /home/test/testdata/models/Meta-Llama-3-8B-Instruct
tokenizer_second_path: /home/test/testdata/models/Meta-Llama-3-8B-Instruct
explore_count: 10
thread_count: 128
prompt_pool_path: ./utils/prompts_math_first.jsonl
# sft
origin_sft_yaml_path: ./alignment-handbook/recipes/Llama3-8b/math_sft_dpo/sft/base.yaml
mid_sft_jsonl_root_path: ./results/math_sft_dpo/sft
mid_sft_dataset_root_path: ./my_datasets/math_sft_dpo/sft
initial_episilon: 0.6
sample_count: 7000
# dpo
origin_dpo_yaml_path: ./alignment-handbook/recipes/Llama3-8b/math_sft_dpo/dpo/base.yaml
mid_dpo_jsonl_root_path: ./results/math_sft_dpo/dpo
mid_dpo_dataset_root_path: ./my_datasets/math_sft_dpo/dpo
initial_dpo_min_value: 0.2
initial_dpo_episilon: 0.45
monte_sample_count: 7000
cal_ppl: 1
lambda1: -0.4
lambda2: 0.9
from_initial: True